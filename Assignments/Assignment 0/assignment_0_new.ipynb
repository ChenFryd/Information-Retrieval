{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCas56nQyOqB"
   },
   "source": [
    "# Assignment 0: Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlD96dTNnE-6"
   },
   "source": [
    "\n",
    "**In this assignment we will focus on setting up and get a deeper understanting on the environments we will work with** in practical assignments and in the final class project.\n",
    "\n",
    "\n",
    "### Goals: \n",
    "\n",
    "1.   Get familiar with Google Colab and Google Cloud Platform (GCP).\n",
    "2.   Execute basic code in these environments.\n",
    "3.   Run tests.\n",
    "\n",
    "This assignment is **not graded**, its sole purpose is to help you get set up and comfortable in the environments we will use in subsequent assignments and in class project.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VccPFI-Sp57X"
   },
   "source": [
    "### Tasks: \n",
    "\n",
    "\n",
    "1.   Go through [Colab](https://colab.research.google.com/notebooks/intro.ipynb?utm_source=scs-index) and [GCP](https://www.ee.columbia.edu/~cylin/course/bigdata/HW0_tutorial_1.pdf) tutorials. \n",
    "2.   Run the following notebook to read a wikipedia file and split it to articles. \n",
    "3. Write the code to extract the right number of articles. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iZU6j2rksna"
   },
   "source": [
    "### Data\n",
    "\n",
    "In all the coding assigments in the course you will need to use Wikipedia Data. Full data can be found [here](https://dumps.wikimedia.org/enwiki/20210801/), but this data is not preprocessed.\n",
    "To save you (and us) time, in most assignment, and in the project as well, you will use preprocessed files that can be found in a Google Storage Bucket in the link \"gs://wikidata_preprocessed/\". When you will need to use data from the google storage bucket, we will provide the code to read the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAdNnF04t3Hd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bz2\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from itertools import islice\n",
    "from xml.etree import ElementTree\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import gdown\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuDnZdXSsDJj"
   },
   "source": [
    "## Read Wikipedia Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1666614047655,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "eEAagtnOwYK2",
    "outputId": "4e9ef09e-fec5-4479-cae5-d074347f9709"
   },
   "outputs": [],
   "source": [
    "## Download one wikipedia file\n",
    "path_url = 'https://drive.google.com/file/d/1c2ggRHG0WqLmJpIE-HfQgJi73kww3Zyc/view?usp=sharing'\n",
    "wiki_file = 'enwiki-20210801-pages-articles-multistream15.xml-p17324603p17460152.bz2'\n",
    "id = '1c2ggRHG0WqLmJpIE-HfQgJi73kww3Zyc'\n",
    "gdown.download(id=id, output=wiki_file, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666614047655,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "DCMdcQqawjWn",
    "outputId": "3fca9ec4-f427-4edb-8a91-7d8df738173d"
   },
   "outputs": [],
   "source": [
    "# Make sure you downloaded the file\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1666614047917,
     "user": {
      "displayName": "Shahar Shcheranski",
      "userId": "08749577003328595661"
     },
     "user_tz": -180
    },
    "id": "Z0bNWO0zwl-_",
    "outputId": "2f0af25f-a9d2-4d92-e79e-4f1ee57ab273"
   },
   "outputs": [],
   "source": [
    "# Print the first 59 lines of the (uncompressed) file\n",
    "!bzcat $wiki_file | head -n 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-LFvoSzwQ1h"
   },
   "outputs": [],
   "source": [
    "def page_iter(wiki_file):\n",
    "  \"\"\" Reads a wiki dump file and create a generator that yields pages. \n",
    "  Parameters:\n",
    "  -----------\n",
    "  wiki_file: str\n",
    "    A path to wiki dump file.\n",
    "  Returns:\n",
    "  --------\n",
    "  tuple\n",
    "    containing three elements: article id, title, and body. \n",
    "  \"\"\"\n",
    "  # open compressed bz2 dump file\n",
    "  with bz2.open(wiki_file, 'rt', encoding='utf-8', errors='ignore') as f_in:\n",
    "    # Create iterator for xml that yields output when tag closes\n",
    "    elems = (elem for _, elem in ElementTree.iterparse(f_in, events=(\"end\",)))\n",
    "    # Consume the first element and extract the xml namespace from it. \n",
    "    # Although the raw xml has the  short tag names without namespace, i.e. it \n",
    "    # has <page> tags and not <http://wwww.mediawiki.org/xml/export...:page> \n",
    "    # tags, the parser reads it *with* the namespace. Therefore, it needs the \n",
    "    # namespace when looking for child elements in the find function as below.\n",
    "    elem = next(elems)\n",
    "    m = re.match(\"^{(http://www\\.mediawiki\\.org/xml/export-.*?)}\", elem.tag)\n",
    "    if m is None:\n",
    "        raise ValueError(\"Malformed MediaWiki dump\")\n",
    "    ns = {\"ns\": m.group(1)}\n",
    "    page_tag = ElementTree.QName(ns['ns'], 'page').text\n",
    "    # iterate over elements\n",
    "    for elem in elems:\n",
    "      if elem.tag == page_tag:\n",
    "        # Filter out redirect and non-article pages\n",
    "        if elem.find('./ns:redirect', ns) is not None or \\\n",
    "           elem.find('./ns:ns', ns).text != '0':\n",
    "          elem.clear()\n",
    "          continue\n",
    "        # Extract the article wiki id\n",
    "        wiki_id = elem.find('./ns:id', ns).text\n",
    "        # Extract the article title into a variables called title\n",
    "        # None for now, you will be asked to extract the title in assignment 1.\n",
    "        title = None\n",
    "        # extract body\n",
    "        body = elem.find('./ns:revision/ns:text', ns).text\n",
    "\n",
    "        yield wiki_id, title, body\n",
    "        elem.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnYWxAx52Iv_"
   },
   "source": [
    "**YOUR TASK** (Non Graded): Complete the following function that returns the number of pages in the dump file we read. Code parts for implementation are marked with a comment `# YOUR CODE HERE`. The last cell is a test to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Jlt-UUCAufJu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f755f2112107fb5f38a4f164a3ae6fc",
     "grade": false,
     "grade_id": "cell-a9af20c4e61dc58a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def count_pages():\n",
    "  \"\"\" Returns a count of the number of pages yielded by `page_iter`. \"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JhGEQapE0aX3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9afc38e2bfb0f8161f58ae039c0b5a0",
     "grade": true,
     "grade_id": "cell-946a81460c318414",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 10974 == count_pages()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
