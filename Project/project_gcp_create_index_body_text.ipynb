{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00e032c",
   "metadata": {
    "id": "a00e032c"
   },
   "source": [
    "***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac36d3a",
   "metadata": {
    "id": "5ac36d3a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Worker_Count",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
      "cluster-dfa7  GCE       2                                             RUNNING  us-central1-a\r\n"
     ]
    }
   ],
   "source": [
    "# if the following command generates an error, you probably didn't enable\n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security â†’ Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf86c5",
   "metadata": {
    "id": "51cf86c5"
   },
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf199e6a",
   "metadata": {
    "id": "bf199e6a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Setup",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f56ecd",
   "metadata": {
    "id": "d8f56ecd",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import math\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a897f2",
   "metadata": {
    "id": "38a897f2",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-jar",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "8f93a7ec-71e0-49c1-fc81-9af385849a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 247882 Mar  7 13:05 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47900073",
   "metadata": {
    "id": "47900073",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-pyspark-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980e62a5",
   "metadata": {
    "id": "980e62a5",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bucket_name",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Put your bucket name below and make sure you can access it without an error\n",
    "bucket_name = 'bucket_title'\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name.endswith('parquet'):\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c3f5e",
   "metadata": {
    "id": "582c3f5e"
   },
   "source": [
    "# Building an inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c523e7",
   "metadata": {
    "id": "e4c523e7",
    "outputId": "71332954-db7d-472c-c79e-3c937d8d92b2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "n_pages = parquetFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "121fe102",
   "metadata": {
    "id": "121fe102",
    "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted_index_gcp.py\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57c101a8",
   "metadata": {
    "id": "57c101a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c259c402",
   "metadata": {
    "id": "c259c402"
   },
   "outputs": [],
   "source": [
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ad8fea",
   "metadata": {
    "id": "f3ad8fea",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-token2bucket",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "def get_tokens(text, doc_id):\n",
    "    \"\"\"Support function - performs token filtering per doc \n",
    "      Parameters:\n",
    "    -----------\n",
    "    text: str\n",
    "      Text of one document\n",
    "    id: int\n",
    "      Document id\n",
    "    Returns:\n",
    "    --------\n",
    "    List of tuples\n",
    "      A list of (doc_id, filtered_token_list) pairs \n",
    "    \"\"\"\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token != '']\n",
    "    filtered_tokens = [tok for tok in tokens if (tok not in all_stopwords)]\n",
    "    return [(doc_id, filtered_tokens)]\n",
    "\n",
    "def calculate_df(postings):\n",
    "    ''' Takes a posting list RDD and calculate the df for each token.\n",
    "    Parameters:\n",
    "    -----------\n",
    "      postings: RDD\n",
    "        An RDD where each element is a (token, posting_list) pair.\n",
    "    Returns:\n",
    "    --------\n",
    "      RDD\n",
    "        An RDD where each element is a (token, df) pair.\n",
    "    '''\n",
    "    # Count the number of documents for each token\n",
    "    df_rdd = postings.map(lambda x: (x[0], len(x[1])))\n",
    "\n",
    "    return df_rdd\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    ''' Returns a sorted posting list by wiki_id.\n",
    "    Parameters:\n",
    "    -----------\n",
    "      unsorted_pl: list of tuples\n",
    "        A list of (wiki_id, tf) tuples\n",
    "    Returns:\n",
    "    --------\n",
    "      list of tuples\n",
    "        A sorted posting list.\n",
    "    '''\n",
    "    sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])  # Sort by wiki_id\n",
    "    return sorted_pl\n",
    "\n",
    "def word_count(tokens, id):\n",
    "    ''' Count the frequency of each word in `text` (tf) that is not included in\n",
    "    `all_stopwords` and return entries that will go into our posting lists.\n",
    "    Parameters:\n",
    "    -----------\n",
    "      text: str\n",
    "        Text of one document\n",
    "      id: int\n",
    "        Document id\n",
    "    Returns:\n",
    "    --------\n",
    "      List of tuples\n",
    "        A list of (token, (doc_id, tf)) pairs\n",
    "        for example: [(\"Anarchism\", (12, 5)), ...]\n",
    "    '''\n",
    "    doc_word_count = Counter(tokens)\n",
    "    return [(tok, (id, tf)) for tok, tf in doc_word_count.items()]\n",
    "\n",
    "def get_doc_len(doc_id, tokens):\n",
    "    \"\"\" Count document filtered length for storage in index as well as document vector length for RDD calculations\n",
    "  Parameters:\n",
    "  -----------\n",
    "    id: int\n",
    "      Document id\n",
    "    tokens: str\n",
    "      list of tokens from document\n",
    "    Returns:\n",
    "  --------\n",
    "    List of tuples\n",
    "      A list of (doc_id, doc_length) pairs\n",
    "  \"\"\"\n",
    "    doc_word_count = Counter(tokens)\n",
    "    doc_len = sum(doc_word_count.values())\n",
    "    return [(doc_id, doc_len)]\n",
    "    \n",
    "def partition_postings_and_write(postings,base_dir):\n",
    "    ''' A function that partitions the posting lists into buckets, writes out\n",
    "    all posting lists in a bucket to disk, and returns the posting locations for\n",
    "    each bucket. Partitioning should be done through the use of `token2bucket`\n",
    "    above. Writing to disk should use the function  `write_a_posting_list`, a\n",
    "    static method implemented in inverted_index_colab.py under the InvertedIndex\n",
    "    class.\n",
    "    Parameters:\n",
    "    -----------\n",
    "      postings: RDD\n",
    "        An RDD where each item is a (w, posting_list) pair.\n",
    "    Returns:\n",
    "    --------\n",
    "      RDD\n",
    "        An RDD where each item is a posting locations dictionary for a bucket. The\n",
    "        posting locations maintain a list for each word of file locations and\n",
    "        offsets its posting list was written to. See `write_a_posting_list` for\n",
    "        more details.\n",
    "    '''\n",
    "    # Step 1: Partition the posting lists into buckets\n",
    "    buckets = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))\n",
    "\n",
    "    # Step 2: Group posting lists by bucket ID\n",
    "    grouped_buckets = buckets.groupByKey()\n",
    "\n",
    "    # Step 3 & 4: Write each bucket's posting lists to disk and collect information about their location\n",
    "    posting_locations = grouped_buckets.map(lambda x: InvertedIndex.write_a_posting_list(x,base_dir,bucket_name))\n",
    "\n",
    "    return posting_locations\n",
    "\n",
    "def get_tfidf(tf, df, doc_len):\n",
    "    if doc_len == 0:\n",
    "        return 0.0\n",
    "    tf_idf = tf/doc_len * math.log(n_pages / (df + 1), 2)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe23d09c7883f8f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "index_name = \"full_body_index\"\n",
    "doc_tok = doc_text_pairs.flatMap(lambda x: get_tokens(x[0], x[1]))\n",
    "\n",
    "# calculate document length for later tf normalization\n",
    "doc_length = doc_tok.flatMap(lambda x: get_doc_len(x[1], x[0]))\n",
    "# calculate term frequency by document\n",
    "word_counts = doc_tok.flatMap(lambda x: word_count(x[1], x[0]))\n",
    "print(\"running bodyIndex with uncommon_words filter\\n\")\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts).filter(lambda x: len(x[1]) >= 50)\n",
    "# Calculate term document frequency\n",
    "w2df = calculate_df(postings)\n",
    "\n",
    "# Calculate sqrt norm of each document - get tf from posting, df from w2df, doc length and calculate tfidf^2 per doc_id, term and sum by doc_id\n",
    "doc2norm = postings.flatMapValues(lambda x: x).leftOuterJoin(w2df) \\\n",
    "     .map(lambda x: (x[1][0][0], (x[0], x[1][0][1], x[1][1]))) \\\n",
    "     .leftOuterJoin(doc_length).map(lambda x: (x[0], (x[1][0][1], x[1][0][2], x[1][1]))) \\\n",
    "     .mapValues(lambda x: get_tfidf(x[0], x[1], x[2])).mapValues(lambda x: pow(x,2)) \\\n",
    "     .map(lambda x: (x[0], x[1])).reduceByKey(lambda x,y: x+y).mapValues(lambda x: math.sqrt(x))\n",
    "#.mapValues(lambda x: round(x, 6))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save doc_id: (doc_norm, doc_len) RDD for cosin similarity calculations\n",
    "doc_data = doc2norm.join(doc_length)\n",
    "tfidf_location = index_name +\"/tfidf\"\n",
    "_ = partition_postings_and_write(doc_data, tfidf_location).collect()\n",
    "super_doc_data_locs = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix=tfidf_location):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_doc_data_locs[k].extend(v)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "211120571bbb1f7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# write posting to bin files\n",
    "postings_location = index_name+\"/postings\"\n",
    "_ = partition_postings_and_write(postings, postings_location).collect()\n",
    "print(f\"PostingLocs created for {index_name}\\n\")\n",
    "\n",
    "super_posting_locs = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix=postings_location):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs[k].extend(v)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4ff46b70f7f6965"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create inverted index instance\n",
    "inverted = InvertedIndex(base_dir=index_name,name=index_name,bucket_name=bucket_name)\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted.posting_locs = super_posting_locs\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted.df.update(w2df.collectAsMap())\n",
    "print(f\"document frequancy created for {index_name}\\n\")\n",
    "# Count number of docs\n",
    "inverted._N = n_pages\n",
    "# Get each document length and norm\n",
    "inverted.doc_data = super_doc_data_locs\n",
    "print(f\"doc data created for {index_name}\\n\")\n",
    "# save titles to return results\n",
    "# inverted.docID_to_title_dict = id2title\n",
    "# write the global stats out\n",
    "inverted.write_index()\n",
    "print(f\"{index_name} written\\n\")\n",
    "# upload to gs\n",
    "index_src = f\"{index_name}.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{index_name}/{index_src}'\n",
    "!gsutil cp $index_src $index_dst\n",
    "print(f\"{index_name} uploaded to bucket\\n\")\n",
    "\n",
    "!gsutil ls -lh $index_dst"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aa70b4385ddaab0"
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
